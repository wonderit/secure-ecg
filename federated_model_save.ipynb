{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \" \"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import argparse\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-c\", \"--is_comet\", help=\"Set is Comet\", action='store_true')\n",
    "parser.add_argument(\"-m\", \"--model_type\", help=\"model name(shallow, normal, ann, mpc, cnn2d)\", type=str, default='cnnmax')\n",
    "parser.add_argument(\"-mpc\", \"--mpc\", help=\"shallow model\", action='store_true')\n",
    "parser.add_argument(\"-lt\", \"--loss_type\", help=\"use sgd as optimizer\", type=str, default='adam')\n",
    "parser.add_argument(\"-e\", \"--epochs\", help=\"Set epochs\", type=int, default=10)\n",
    "parser.add_argument(\"-b\", \"--batch_size\", help=\"Set batch size\", type=int, default=32)\n",
    "parser.add_argument(\"-lr\", \"--lr\", help=\"Set learning rate\", type=float, default=1e-2)\n",
    "parser.add_argument(\"-eps\", \"--eps\", help=\"Set epsilon of adam\", type=float, default=1e-7)\n",
    "parser.add_argument(\"-s\", \"--seed\", help=\"Set random seed\", type=int, default=1234)\n",
    "parser.add_argument(\"-sc\", \"--scaler\", help=\"Set random seed\", type=str, default='max30_federated')\n",
    "parser.add_argument(\"-li\", \"--log_interval\", help=\"Set log interval\", type=int, default=5)\n",
    "parser.add_argument(\"-mom\", \"--momentum\", help=\"Set momentum\", type=float, default=0.9)\n",
    "parser.add_argument(\"-fr\", \"--federated_ratio\", help=\"federated_ratio\", type=float, default=0.01)\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "max_x = 0\n",
    "\n",
    "def scale(arr, m, s):\n",
    "    arr = arr - m\n",
    "    arr = arr / (s + 1e-7)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def rescale(arr, m, s):\n",
    "    arr = arr * s\n",
    "    arr = arr + m\n",
    "    return arr\n",
    "\n",
    "\n",
    "def scale_minmax(arr, min, max):\n",
    "    arr = (arr - min) / (max - min)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def scale_maxabs(arr, maxabs):\n",
    "    arr = arr / maxabs\n",
    "    return arr\n",
    "\n",
    "\n",
    "def scale_robust(arr, q1, q3):\n",
    "    print('q1 : ', q1, 'q1 : ', q3)\n",
    "    arr = (arr - q1) / (q3-q1)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def return_maxabs_min_max(arr, q1, q3):\n",
    "    print('q1 : ', q1, 'q1 : ', q3)\n",
    "    arr = (arr - q1) / (q3-q1)\n",
    "    return arr\n",
    "\n",
    "MEAN = 61.9\n",
    "STD = 10.9\n",
    "\n",
    "_ = torch.manual_seed(args.seed)\n",
    "\n",
    "result_path = os.path.join('result_torch', 'fed_{}_fr{}_{}_{}_eps{}_ep{}_bs{}_lr{}_mom{}'.format(\n",
    "    args.scaler,\n",
    "    args.federated_ratio,\n",
    "    args.model_type,\n",
    "    args.loss_type,\n",
    "    args.eps,\n",
    "    args.epochs,\n",
    "    args.batch_size,\n",
    "    args.lr,\n",
    "    args.momentum\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to TorchDataset...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batches = 5000 / args.batch_size\n",
    "log_batches = int(batches / args.log_interval)\n",
    "\n",
    "DATAPATH = '../data/ecg/text_demo_5500'\n",
    "train_file_suffix = 'train'\n",
    "test_file_suffix = 'test'\n",
    "\n",
    "file_name_train_x = 'X{}'.format(train_file_suffix)\n",
    "file_name_train_y = 'y{}'.format(train_file_suffix)\n",
    "file_name_test_x = 'X{}'.format(test_file_suffix)\n",
    "file_name_test_y = 'y{}'.format(test_file_suffix)\n",
    "\n",
    "print('Converting to TorchDataset...')\n",
    "\n",
    "train_x = np.loadtxt('{}/{}'.format(DATAPATH, file_name_train_x), delimiter=',')\n",
    "test_x = np.loadtxt('{}/{}'.format(DATAPATH, file_name_test_x), delimiter=',')\n",
    "\n",
    "total_x = np.vstack((train_x, test_x))\n",
    "\n",
    "train_y = np.loadtxt('{}/{}'.format(DATAPATH, file_name_train_y), delimiter=',')\n",
    "test_y = np.loadtxt('{}/{}'.format(DATAPATH, file_name_test_y), delimiter=',')\n",
    "\n",
    "train_x = train_x.reshape(train_x.shape[0], 3, 500)\n",
    "test_x = test_x.reshape(test_x.shape[0], 3, 500)\n",
    "\n",
    "result_array = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "federated_instance_count =  int(train_x.shape[0] * args.federated_ratio)\n",
    "train_x = train_x[:federated_instance_count, :, :]\n",
    "train_y = train_y[:federated_instance_count]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Dataset Train/Test split finished...\n"
     ]
    }
   ],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.target = torch.from_numpy(target).float()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(data, [args.n_train_items, args.n_test_items])\n",
    "train_dataset = ECGDataset(train_x, train_y, transform=False)\n",
    "test_dataset = ECGDataset(test_x, test_y, transform=False)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "print('Torch Dataset Train/Test split finished...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "class CNNAVG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNAVG, self).__init__()\n",
    "        self.kernel_size = 7\n",
    "        self.padding_size = 0\n",
    "        self.channel_size = 6\n",
    "        self.avgpool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        self.avgpool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        self.avgpool3 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv1d(3, self.channel_size, kernel_size=self.kernel_size,\n",
    "                               padding=(self.kernel_size // 2))\n",
    "        self.conv2 = nn.Conv1d(self.channel_size, self.channel_size, kernel_size=self.kernel_size,\n",
    "                               padding=(self.kernel_size // 2))\n",
    "        self.conv3 = nn.Conv1d(self.channel_size, self.channel_size, kernel_size=self.kernel_size,\n",
    "                               padding=(self.kernel_size // 2))\n",
    "        self.fc1 = nn.Linear(372, 16)\n",
    "        self.fc2 = nn.Linear(16, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        # self.max_x = max_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # 32\n",
    "        x = self.avgpool1(x)  # 32\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.avgpool2(x)\n",
    "        y = F.relu(self.conv3(x))\n",
    "\n",
    "        y = self.avgpool3(y)\n",
    "        y = y.view(y.shape[0], -1)\n",
    "        y = F.relu(self.fc1(y))\n",
    "\n",
    "        y = F.relu(self.fc2(y))\n",
    "\n",
    "        y = self.fc3(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class CNNMAX(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNMAX, self).__init__()\n",
    "        self.kernel_size = 7\n",
    "        self.padding_size = 0\n",
    "        self.channel_size = 6\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.maxpool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv1d(3, self.channel_size, kernel_size=self.kernel_size,\n",
    "                               padding=(self.kernel_size // 2))\n",
    "        self.conv2 = nn.Conv1d(self.channel_size, self.channel_size, kernel_size=self.kernel_size,\n",
    "                               padding=(self.kernel_size // 2))\n",
    "        self.conv3 = nn.Conv1d(self.channel_size, self.channel_size, kernel_size=self.kernel_size,\n",
    "                               padding=(self.kernel_size // 2))\n",
    "        self.fc1 = nn.Linear(372, 16)\n",
    "        self.fc2 = nn.Linear(16, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # 32\n",
    "        x = self.maxpool1(x)  # 32\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.maxpool2(x)\n",
    "        y = F.relu(self.conv3(x))\n",
    "        y = self.maxpool3(y)\n",
    "        y = y.view(y.shape[0], -1)\n",
    "\n",
    "        y = F.relu(self.fc1(y))\n",
    "        y = F.relu(self.fc2(y))\n",
    "        y = self.fc3(y)\n",
    "        return y\n",
    "\n",
    "def report_scores(X, y, trained_model):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = trained_model(torch.from_numpy(X).float())\n",
    "        # output rescale\n",
    "        scores = rescale(scores, MEAN, STD)\n",
    "        y = rescale(y, MEAN, STD)\n",
    "\n",
    "        mse_loss = mean_squared_error(y, scores)\n",
    "\n",
    "        y_true.extend(list(y))\n",
    "        y_pred.extend(scores)\n",
    "\n",
    "    return y_true, y_pred, mse_loss\n",
    "\n",
    "\n",
    "def train(args, model, private_train_loader, optimizer, epoch, test_loader):\n",
    "    training_step = log_batches * (epoch-1)\n",
    "    model.train()\n",
    "    data_count = 0\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(private_train_loader):  # <-- now it is a private dataset\n",
    "        # if target.min() < 25 or target.max() > 140:\n",
    "        #     continue\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # loss = F.nll_loss(output, target)  <-- not possible here\n",
    "        batch_size = output.shape[0]\n",
    "\n",
    "        # Reshape\n",
    "        # output = output.view(-1)\n",
    "        # target = target.view(-1)\n",
    "\n",
    "        target = target.view(target.shape[0], 1)\n",
    "        # r2 : 0.67 with smooth l1 loss\n",
    "        # loss = F.smooth_l1_loss(output, target).sum() / batch_size\n",
    "\n",
    "        # r2 : 0.7  with logcosh loss\n",
    "        # loss = (torch.log(torch.cosh(output - target))).sum() / batch_size\n",
    "\n",
    "        # r2 : 0.646 w mse loss\n",
    "        loss = ((output - target) ** 2).sum() / batch_size\n",
    "        # loss = ((output - target) ** 2).sum()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss = loss.item()\n",
    "        data_count += 1\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            # loss = loss.get().float_precision()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.3f}s'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(private_train_loader) * args.batch_size,\n",
    "                       100. * batch_idx / len(private_train_loader), loss.item(), time.time() - start_time))\n",
    "\n",
    "            # if args.is_comet:\n",
    "            #     training_step = training_step + 1\n",
    "            #     y_true_train, y_pred_train, train_mse_loss = report_scores(train_x, train_y, model)\n",
    "            #     _, train_r = r_mse(y_true_train, y_pred_train, train_mse_loss)\n",
    "            #     print('step : ', training_step)\n",
    "            #     experiment.log_metric(\"train_mse\", train_mse_loss, epoch=epoch, step=training_step)\n",
    "            #     experiment.log_metric(\"train_r\", train_r, epoch=epoch, step=training_step)\n",
    "\n",
    "            # test during training\n",
    "            test(args, model, test_loader, epoch, batch_idx, training_step)\n",
    "\n",
    "def test(args, model, private_test_loader, epoch, batch=999, step=0):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    data_count = 0\n",
    "    pred_list = []\n",
    "    target_list = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in private_test_loader:\n",
    "            start_time = time.time()\n",
    "\n",
    "            output = model(data)\n",
    "            #\n",
    "            # output rescale\n",
    "            output = rescale(output, MEAN, STD)\n",
    "            target = rescale(target, MEAN, STD)\n",
    "\n",
    "            # Reshape\n",
    "            # output = output.view(-1)\n",
    "            # target = target.view(-1)\n",
    "\n",
    "            target = target.view(target.shape[0], 1)\n",
    "\n",
    "            test_loss += ((output - target) ** 2).sum()\n",
    "            # test_loss += torch.log(torch.cosh(output - target)).sum()\n",
    "\n",
    "            data_count += len(output)\n",
    "            pred_list.extend(output.numpy())\n",
    "            target_list.extend(target.numpy())\n",
    "            # print('rmse:', torch.sqrt(((output - target) ** 2).sum() / args.batch_size))\n",
    "            # print('r2score:', r2_score(target_list, pred_list))\n",
    "\n",
    "    # test_loss = test_loss.get().float_precision()\n",
    "    # print('Test set: Loss: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.3f}s'.format(batch_idx * args.batch_size, len(private_train_loader) * args.batch_size,\n",
    "    #            100. * batch_idx / len(private_train_loader), loss.item(), time.time() - start_time))\n",
    "    print('\\nTest set: Loss: avg MSE ({:.4f})\\tTime: {:.3f}s'.format(test_loss / data_count, time.time() - start_time))\n",
    "\n",
    "\n",
    "    # # output rescale\n",
    "    # target_list = rescale(target_list, MEAN, STD)\n",
    "    # pred_list = rescale(pred_list, MEAN, STD)\n",
    "\n",
    "    rm, test_r = r_mse(target_list, pred_list)\n",
    "\n",
    "    if batch % 155 == 0:\n",
    "        scatter_plot(target_list, pred_list, epoch, rm, batch)\n",
    "    #\n",
    "    #\n",
    "    # if args.is_comet:\n",
    "    #     experiment.log_metric(\"test_mse\", test_loss / data_count, epoch=epoch, step=step)\n",
    "    #     experiment.log_metric(\"test_r\", test_r, epoch=epoch, step=step)\n",
    "\n",
    "    if batch % args.log_interval == 0:\n",
    "        result = dict()\n",
    "        result['mse_test'] = test_loss.numpy() / data_count\n",
    "        result['r_test'] = test_r\n",
    "        result_array.append(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNMAX(\n",
      "  (maxpool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (maxpool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (maxpool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv1): Conv1d(3, 6, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (conv2): Conv1d(6, 6, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (conv3): Conv1d(6, 6, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "  (fc1): Linear(in_features=372, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "p: (6, 3, 7) 3\n",
      "p: (6,) 1\n",
      "p: (6, 6, 7) 3\n",
      "p: (6,) 1\n",
      "p: (6, 6, 7) 3\n",
      "p: (6,) 1\n",
      "p: (16, 372) 2\n",
      "p: (16,) 1\n",
      "p: (64, 16) 2\n",
      "p: (64,) 1\n",
      "p: (1, 64) 2\n",
      "p: (1,) 1\n",
      "Train Epoch: 1 [0/64 (0%)]\tLoss: 1.723828\tTime: 0.016s\n",
      "\n",
      "Test set: Loss: avg MSE (89.5782)\tTime: 0.002s\n",
      "Scoring - std 9.308054141185021 0.07381448898319282\n",
      "Scoring - median 60.99966049194336 63.16550636291504\n",
      "Scoring - min 30.000062942504883 62.789093017578125\n",
      "Scoring - max 99.99986267089844 63.46007537841797\n",
      "Scoring - mean 61.45395826339722 63.163060539245606\n",
      "Scoring - MSE:  89.57825 RMSE:  9.464578546893279\n",
      "Scoring - R2:  7.504332572517672e-05\n",
      "Train Epoch: 2 [0/64 (0%)]\tLoss: 1.713797\tTime: 0.008s\n",
      "\n",
      "Test set: Loss: avg MSE (89.7279)\tTime: 0.002s\n",
      "Scoring - std 9.308054141185021 0.07761080523637061\n",
      "Scoring - median 60.99966049194336 63.247745513916016\n",
      "Scoring - min 30.000062942504883 62.87884521484375\n",
      "Scoring - max 99.99986267089844 63.652442932128906\n",
      "Scoring - mean 61.45395826339722 63.253231826782226\n",
      "Scoring - MSE:  89.72791 RMSE:  9.47248187661671\n",
      "Scoring - R2:  0.011563266112527053\n",
      "Train Epoch: 3 [0/64 (0%)]\tLoss: 1.708582\tTime: 0.008s\n",
      "\n",
      "Test set: Loss: avg MSE (89.8714)\tTime: 0.002s\n",
      "Scoring - std 9.308054141185021 0.0894904723147771\n",
      "Scoring - median 60.99966049194336 63.33274269104004\n",
      "Scoring - min 30.000062942504883 62.973777770996094\n",
      "Scoring - max 99.99986267089844 63.80604934692383\n",
      "Scoring - mean 61.45395826339722 63.34217917633057\n",
      "Scoring - MSE:  89.871414 RMSE:  9.48005349059647\n",
      "Scoring - R2:  0.04210308993209254\n",
      "Train Epoch: 4 [0/64 (0%)]\tLoss: 1.702679\tTime: 0.007s\n",
      "\n",
      "Test set: Loss: avg MSE (90.0466)\tTime: 0.002s\n",
      "Scoring - std 9.308054141185021 0.11032872966380598\n",
      "Scoring - median 60.99966049194336 63.42972373962402\n",
      "Scoring - min 30.000062942504883 63.0808219909668\n",
      "Scoring - max 99.99986267089844 64.00809478759766\n",
      "Scoring - mean 61.45395826339722 63.434488059997555\n",
      "Scoring - MSE:  90.04659 RMSE:  9.489288314325913\n",
      "Scoring - R2:  0.06607433653618265\n",
      "Train Epoch: 5 [0/64 (0%)]\tLoss: 1.694966\tTime: 0.008s\n",
      "\n",
      "Test set: Loss: avg MSE (90.2303)\tTime: 0.002s\n",
      "Scoring - std 9.308054141185021 0.14023250588719055\n",
      "Scoring - median 60.99966049194336 63.52743148803711\n",
      "Scoring - min 30.000062942504883 63.16746139526367\n",
      "Scoring - max 99.99986267089844 64.22254180908203\n",
      "Scoring - mean 61.45395826339722 63.537973258972166\n",
      "Scoring - MSE:  90.2303 RMSE:  9.49896314885579\n",
      "Scoring - R2:  0.08752955718288559\n",
      "Train Epoch: 6 [0/64 (0%)]\tLoss: 1.681544\tTime: 0.008s\n",
      "\n",
      "Test set: Loss: avg MSE (90.3878)\tTime: 0.002s\n",
      "Scoring - std 9.308054141185021 0.18111947939179107\n",
      "Scoring - median 60.99966049194336 63.61765670776367\n",
      "Scoring - min 30.000062942504883 63.17942810058594\n",
      "Scoring - max 99.99986267089844 64.3505859375\n",
      "Scoring - mean 61.45395826339722 63.63624764251709\n",
      "Scoring - MSE:  90.387825 RMSE:  9.507251180662422\n",
      "Scoring - R2:  0.09646690897766247\n",
      "Train Epoch: 7 [0/64 (0%)]\tLoss: 1.669725\tTime: 0.008s\n",
      "\n",
      "Test set: Loss: avg MSE (90.6463)\tTime: 0.002s\n",
      "Scoring - std 9.308054141185021 0.23873374510677864\n",
      "Scoring - median 60.99966049194336 63.740760803222656\n",
      "Scoring - min 30.000062942504883 63.23796463012695\n",
      "Scoring - max 99.99986267089844 64.71001434326172\n",
      "Scoring - mean 61.45395826339722 63.77160829925537\n",
      "Scoring - MSE:  90.64634 RMSE:  9.520837117423232\n",
      "Scoring - R2:  0.10237962278336721\n",
      "Train Epoch: 8 [0/64 (0%)]\tLoss: 1.652372\tTime: 0.008s\n",
      "\n",
      "Test set: Loss: avg MSE (91.2375)\tTime: 0.002s\n",
      "Scoring - std 9.308054141185021 0.31887319623375615\n",
      "Scoring - median 60.99966049194336 63.9674072265625\n",
      "Scoring - min 30.000062942504883 63.31922912597656\n",
      "Scoring - max 99.99986267089844 65.30597686767578\n",
      "Scoring - mean 61.45395826339722 64.01368463134766\n",
      "Scoring - MSE:  91.23752 RMSE:  9.551833243443212\n",
      "Scoring - R2:  0.11998632030389739\n",
      "Train Epoch: 9 [0/64 (0%)]\tLoss: 1.635585\tTime: 0.014s\n",
      "\n",
      "Test set: Loss: avg MSE (92.2222)\tTime: 0.003s\n",
      "Scoring - std 9.308054141185021 0.4201384624144406\n",
      "Scoring - median 60.99966049194336 64.26200103759766\n",
      "Scoring - min 30.000062942504883 63.40043640136719\n",
      "Scoring - max 99.99986267089844 65.98021697998047\n",
      "Scoring - mean 61.45395826339722 64.32836791229248\n",
      "Scoring - MSE:  92.22215 RMSE:  9.603236574715888\n",
      "Scoring - R2:  0.1333810853633558\n",
      "Train Epoch: 10 [0/64 (0%)]\tLoss: 1.617157\tTime: 0.008s\n",
      "\n",
      "Test set: Loss: avg MSE (93.3984)\tTime: 0.002s\n",
      "Scoring - std 9.308054141185021 0.5313578959441974\n",
      "Scoring - median 60.99966049194336 64.57837295532227\n",
      "Scoring - min 30.000062942504883 63.45891571044922\n",
      "Scoring - max 99.99986267089844 66.66073608398438\n",
      "Scoring - mean 61.45395826339722 64.65259852600097\n",
      "Scoring - MSE:  93.398384 RMSE:  9.664283941101807\n",
      "Scoring - R2:  0.1441115217300361\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def scatter_plot(y_true, y_pred, epoch, message, batch):\n",
    "    result = np.column_stack((y_true,y_pred))\n",
    "\n",
    "    if not os.path.exists('{}/{}'.format(result_path, 'csv')):\n",
    "        os.makedirs('{}/{}'.format(result_path, 'csv'))\n",
    "\n",
    "    if not os.path.exists('{}/{}'.format(result_path, 'scatter')):\n",
    "        os.makedirs('{}/{}'.format(result_path, 'scatter'))\n",
    "\n",
    "    pd.DataFrame(result).to_csv(\"{}/csv/{}.csv\".format(result_path, epoch), index=False)\n",
    "\n",
    "    import matplotlib.lines as mlines\n",
    "    fig, ax = plt.subplots()\n",
    "    line = mlines.Line2D([0, 1], [0, 1], color='red')\n",
    "\n",
    "    ax.scatter(y_pred, y_true, s=3)\n",
    "\n",
    "    transform = ax.transAxes\n",
    "    line.set_transform(transform)\n",
    "    ax.add_line(line)\n",
    "\n",
    "    plt.suptitle(message)\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('Actual')\n",
    "    # set axes range\n",
    "    plt.xlim(30, 110)\n",
    "    plt.ylim(30, 110)\n",
    "\n",
    "    # plt.savefig(\"{}/scatter/{}.png\".format(result_path, epoch))\n",
    "\n",
    "    if args.is_comet:\n",
    "        experiment.log_figure(figure=plt, figure_name='{}_{}.png'.format(epoch, batch))\n",
    "    else:\n",
    "        plt.savefig(\"{}/scatter/{}_{}.png\".format(result_path, epoch, batch), dpi=600)\n",
    "    plt.clf()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def r_mse(y_true, y_pred, sample_weight=None, multioutput=None):\n",
    "\n",
    "    # r2 = r2_score(y_true, y_pred, multioutput='uniform_average')\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    # bounds_check = np.min(y_pred) > MIN_MOISTURE_BOUND\n",
    "    # bounds_check = bounds_check&(np.max(y_pred) < MAX_MOISTURE_BOUND)\n",
    "\n",
    "    y_true = np.array(y_true, dtype=np.float)\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = np.array(y_pred, dtype=np.float)\n",
    "    y_pred = y_pred.flatten()\n",
    "\n",
    "    r = stats.pearsonr(y_true, y_pred)[0]\n",
    "    r2 = r**2\n",
    "\n",
    "    print('Scoring - std', np.std(y_true), np.std(y_pred))\n",
    "    print('Scoring - median', np.median(y_true), np.median(y_pred))\n",
    "    print('Scoring - min', np.min(y_true), np.min(y_pred))\n",
    "    print('Scoring - max', np.max(y_true), np.max(y_pred))\n",
    "    print('Scoring - mean', np.mean(y_true), np.mean(y_pred))\n",
    "    print('Scoring - MSE: ', mse, 'RMSE: ', math.sqrt(mse))\n",
    "    print('Scoring - R2: ', r2)\n",
    "    # print(y_pred)\n",
    "\n",
    "\n",
    "    result_message = 'r:{:.3f}, mse:{:.3f}, std:{:.3f},{:.3f}'.format(r, mse, np.std(y_true), np.std(y_pred))\n",
    "    return result_message, r\n",
    "\n",
    "def save_model(model, path):\n",
    "\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def transform_array(torch_array, ):\n",
    "    p = torch_array.detach().numpy()\n",
    "    print('p:', p.shape, p.ndim)\n",
    "    # if conv1d\n",
    "    if p.ndim == 3:\n",
    "        p = p.reshape(p.shape[0], -1)\n",
    "    p = np.transpose(p)\n",
    "    p = np.round_(p, 7)\n",
    "    return p\n",
    "\n",
    "def save_model_to_txt(model, path, ep):\n",
    "\n",
    "    conv1_weight = transform_array(model.conv1.weight)\n",
    "    conv1_bias = transform_array(model.conv1.bias)\n",
    "\n",
    "    conv2_weight = transform_array(model.conv2.weight)\n",
    "    conv2_bias = transform_array(model.conv2.bias)\n",
    "\n",
    "\n",
    "    conv3_weight = transform_array(model.conv3.weight)\n",
    "    conv3_bias = transform_array(model.conv3.bias)\n",
    "\n",
    "    fc1_weight = transform_array(model.fc1.weight)\n",
    "    fc1_bias = transform_array(model.fc1.bias)\n",
    "\n",
    "    fc2_weight = transform_array(model.fc2.weight)\n",
    "    fc2_bias = transform_array(model.fc2.bias)\n",
    "\n",
    "    fc3_weight = transform_array(model.fc3.weight)\n",
    "    fc3_bias = transform_array(model.fc3.bias)\n",
    "\n",
    "    np.savetxt('{}ecg_P1_{}_0_W0.bin'.format(path, ep), conv1_weight, fmt='%1.7f')\n",
    "    np.savetxt('{}ecg_P1_{}_0_b0.bin'.format(path, ep), conv1_bias, fmt='%1.7f')\n",
    "    np.savetxt('{}ecg_P1_{}_0_W1.bin'.format(path, ep), conv2_weight, fmt='%1.7f')\n",
    "    np.savetxt('{}ecg_P1_{}_0_b1.bin'.format(path, ep), conv2_bias, fmt='%1.7f')\n",
    "\n",
    "    np.savetxt('{}ecg_P1_{}_0_W2.bin'.format(path, ep), conv3_weight, fmt='%1.7f')\n",
    "    np.savetxt('{}ecg_P1_{}_0_b2.bin'.format(path, ep), conv3_bias, fmt='%1.7f')\n",
    "\n",
    "    np.savetxt('{}ecg_P1_{}_0_W3.bin'.format(path, ep), fc1_weight, fmt='%1.7f')\n",
    "    np.savetxt('{}ecg_P1_{}_0_b3.bin'.format(path, ep), fc1_bias, fmt='%1.7f')\n",
    "    np.savetxt('{}ecg_P1_{}_0_W4.bin'.format(path, ep), fc2_weight, fmt='%1.7f')\n",
    "    np.savetxt('{}ecg_P1_{}_0_b4.bin'.format(path, ep), fc2_bias, fmt='%1.7f')\n",
    "\n",
    "    np.savetxt('{}ecg_P1_{}_0_W5.bin'.format(path, ep), fc3_weight, fmt='%1.7f')\n",
    "    np.savetxt('{}ecg_P1_{}_0_b5.bin'.format(path, ep), fc3_bias, fmt='%1.7f')\n",
    "\n",
    "\n",
    "if args.model_type in ['shallow', 'ann', 'cnn2d', 'cann', 'cnnavg', 'cnnmax']:\n",
    "\n",
    "    if args.model_type == 'cnnavg':\n",
    "        model = CNNAVG()\n",
    "    elif args.model_type == 'cnnmax':\n",
    "        model = CNNMAX()\n",
    "\n",
    "    # if args.model_type == 'cnn2d':\n",
    "    #     summary(model, input_size=(3, 500, 1), batch_size=args.batch_size)\n",
    "    # else:\n",
    "    #     summary(model, input_size=(3, 500), batch_size=args.batch_size)\n",
    "# else:\n",
    "#     model = ML4CVD()\n",
    "#     summary(model, input_size=(12, 5000), batch_size=args.batch_size)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# save_model_to_txt(model, \"{}/models/\".format(result_path), 0)\n",
    "# exit(0)\n",
    "# model = model.fix_precision().share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "# for 12channel\n",
    "\n",
    "# for 1 channel\n",
    "# summary(model, input_size =(1, 12, 5000), batch_size=args.batch_size)\n",
    "# exit()\n",
    "\n",
    "if args.loss_type == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr, eps=args.eps)  # 4.58\n",
    "elif args.loss_type == 'asgd':\n",
    "    optimizer = optim.ASGD(model.parameters(), lr=args.lr)  # 4.58\n",
    "elif args.loss_type == 'lbfgs':\n",
    "    optimizer = optim.LBFGS(model.parameters(), lr=args.lr)  # 4.58\n",
    "elif args.loss_type == 'adadelta':\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)  # 4.58\n",
    "else:\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, nesterov=True)\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "\n",
    "    # Save model\n",
    "    if not os.path.exists('{}/{}'.format(result_path, 'models')):\n",
    "        os.makedirs('{}/{}'.format(result_path, 'models'))\n",
    "\n",
    "    if epoch == 1:\n",
    "        save_model_to_txt(model, \"{}/models/\".format(result_path), epoch-1)\n",
    "    train(args, model, train_loader, optimizer, epoch, test_loader)\n",
    "    # test(args, model, test_loader, epoch, epoch * batches)\n",
    "    if epoch % args.log_interval == 0:\n",
    "        save_model(model, \"{}/models/ep{}.h5\".format(result_path, epoch))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file = \"result_plaintext_adam_changed_{}_{}.csv\".format(args.model_type, args.loss_type)\n",
    "csv_columns = ['mse_test', 'r_test']\n",
    "try:\n",
    "    with open(csv_file, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for data in result_array:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}